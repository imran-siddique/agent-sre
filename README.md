<div align="center">

# Agent SRE

**Reliability Engineering for AI Agent Systems**

*SLOs Â· Error Budgets Â· Chaos Testing Â· Progressive Delivery Â· Cost Guardrails*

[![GitHub Stars](https://img.shields.io/github/stars/imran-siddique/agent-sre?style=social)](https://github.com/imran-siddique/agent-sre/stargazers)
[![Sponsor](https://img.shields.io/badge/sponsor-â¤ï¸-ff69b4)](https://github.com/sponsors/imran-siddique)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.10+-blue.svg)](https://python.org)
[![CI](https://github.com/imran-siddique/agent-sre/actions/workflows/ci.yml/badge.svg)](https://github.com/imran-siddique/agent-sre/actions/workflows/ci.yml)
[![Agent-OS Compatible](https://img.shields.io/badge/agent--os-compatible-green.svg)](https://github.com/imran-siddique/agent-os)
[![AgentMesh Compatible](https://img.shields.io/badge/agentmesh-compatible-green.svg)](https://github.com/imran-siddique/agent-mesh)
[![Discussions](https://img.shields.io/github/discussions/imran-siddique/agent-sre)](https://github.com/imran-siddique/agent-sre/discussions)
[![awesome-opentelemetry](https://img.shields.io/badge/awesome--opentelemetry-listed-orange)](https://github.com/magsther/awesome-opentelemetry/pull/24)

> â­ **If this project helps you, please star it!** It helps others discover Agent SRE.

> ğŸ”— **Part of the Agent Ecosystem** â€” Works with [Agent OS](https://github.com/imran-siddique/agent-os) (governance), [AgentMesh](https://github.com/imran-siddique/agent-mesh) (identity & trust), and [Agent Hypervisor](https://github.com/imran-siddique/agent-hypervisor) (runtime sessions)

> ğŸ“¦ **Install the full stack:** `pip install ai-agent-governance[full]` â€” [PyPI](https://pypi.org/project/ai-agent-governance/) | [GitHub](https://github.com/imran-siddique/agent-governance)

[Quick Start](#-quick-start-in-30-seconds) â€¢ [Examples](examples/) â€¢ [Benchmarks](benchmarks/results/BENCHMARKS.md) â€¢ [Docs](docs/) â€¢ [Agent OS](https://github.com/imran-siddique/agent-os) â€¢ [AgentMesh](https://github.com/imran-siddique/agent-mesh) â€¢ [Agent Hypervisor](https://github.com/imran-siddique/agent-hypervisor)

</div>

### Part of the AgentMesh Governance Ecosystem

<p align="center">
  <a href="https://github.com/langgenius/dify-plugins/pull/2060"><img src="https://img.shields.io/badge/Dify-65K_â­_Merged-success?style=flat-square" alt="Dify"></a>
  <a href="https://github.com/run-llama/llama_index/pull/20644"><img src="https://img.shields.io/badge/LlamaIndex-47K_â­_Merged-success?style=flat-square" alt="LlamaIndex"></a>
  <a href="https://github.com/microsoft/agent-lightning/pull/478"><img src="https://img.shields.io/badge/Agent--Lightning-15K_â­_Merged-success?style=flat-square" alt="Agent-Lightning"></a>
  <a href="https://pypi.org/project/langgraph-trust/"><img src="https://img.shields.io/badge/LangGraph-PyPI-blue?style=flat-square" alt="LangGraph"></a>
  <a href="https://pypi.org/project/openai-agents-trust/"><img src="https://img.shields.io/badge/OpenAI_Agents-PyPI-blue?style=flat-square" alt="OpenAI Agents"></a>
  <a href="https://clawhub.ai/imran-siddique/agentmesh-governance"><img src="https://img.shields.io/badge/OpenClaw-ClawHub-purple?style=flat-square" alt="OpenClaw"></a>
</p>

---

## ğŸ“Š By The Numbers

<table>
<tr>
<td align="center"><h3>7</h3><sub>SRE Engines</sub></td>
<td align="center"><h3>9</h3><sub>Chaos Fault Templates</sub></td>
<td align="center"><h3>7</h3><sub>SLI Types</sub></td>
<td align="center"><h3>100%</h3><sub>Test Coverage<br/>on Core Engines</sub></td>
</tr>
</table>

### ğŸ’¡ Why Agent SRE?

> **AI agents fail differently than traditional services.** They hallucinate, exceed cost budgets, and degrade gradually rather than crash. Traditional SRE tools don't understand agent-specific failure modes. Agent SRE brings SLOs, error budgets, chaos testing, and progressive delivery to the AI agent world.

**Built for the $47B AI agent market** â€” the reliability layer that makes autonomous agents production-ready.

---

## âš¡ Quick Start in 30 Seconds

```bash
pip install agent-sre
```

```python
from agent_sre import SLO, ErrorBudget
from agent_sre.slo.indicators import TaskSuccessRate, CostPerTask, HallucinationRate

# Define what "reliable" means for your agent
slo = SLO(
    name="my-agent",
    indicators=[
        TaskSuccessRate(target=0.95, window="24h"),
        CostPerTask(target_usd=0.50, window="24h"),
        HallucinationRate(target=0.05, window="24h"),
    ],
    error_budget=ErrorBudget(total=0.05),
)

# After each agent task
slo.indicators[0].record_task(success=True)
slo.indicators[1].record_cost(cost_usd=0.35)
slo.indicators[2].record_evaluation(hallucinated=False)
slo.record_event(good=True)

# Check health
status = slo.evaluate()  # HEALTHY, WARNING, CRITICAL, or EXHAUSTED
print(f"Budget remaining: {slo.error_budget.remaining_percent:.1f}%")
```

That's it. Your agent now has SLOs, error budgets, and burn rate alerts. [See all examples â†’](examples/)

---

## The Problem

AI agents in production fail differently than traditional services:

| Failure Mode | Traditional Service | AI Agent |
|---|---|---|
| **Crash** | Stack trace, restart | Same â€” but rare |
| **Wrong answer** | N/A | Returns "success" but the answer is wrong |
| **Silent degradation** | Latency spike | Reasoning quality drops, no metric moves |
| **Cost explosion** | Predictable | Runaway tool loops burn $10K in minutes |
| **Cascade failure** | Service A â†’ B | Agent A trusts Agent B who hallucinates |
| **Tool drift** | API versioning | MCP server schema changes silently break workflows |

Your APM dashboard says "HTTP 200, latency 150ms, all green" while your agent just approved a fraudulent transaction.

**Traditional monitoring catches crashes. Agent SRE catches *everything else*.**

## The Solution

Agent SRE brings Site Reliability Engineering to AI agents â€” the same discipline that keeps Google, Netflix, and Spotify reliable, adapted for non-deterministic agent workloads.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Your AI Agents                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Agent SRE â€” The Reliability Lifecycle                          â”‚
â”‚                                                                 â”‚
â”‚  1. DEFINE    SLOs â€” what does "reliable" mean?                  â”‚
â”‚  2. MEASURE   SLIs â€” are we meeting those targets?              â”‚
â”‚  3. PROTECT   Cost Guard + Circuit Breaker â€” prevent disasters  â”‚
â”‚  4. SHIP      Shadow + Canary â€” deploy changes safely           â”‚
â”‚  5. BREAK     Chaos Engine â€” prove resilience before prod does  â”‚
â”‚  6. RESPOND   Incidents + Postmortem â€” recover fast             â”‚
â”‚  7. LEARN     Replay + Diff â€” understand exactly what happened  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  AgentMesh â€” Identity, Trust, Routing                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Agent OS â€” Policy Enforcement, Audit, Compliance               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Core Capabilities

### 1. SLO Engine â€” Define What "Reliable" Means

Traditional SRE defines SLOs for services (99.9% uptime). Agent SRE defines SLOs for *agent behavior*:

| SLI (Indicator) | Example SLO | What It Catches |
|---|---|---|
| **Task Success Rate** | 99.5% of tasks correct | Silent reasoning failures |
| **Tool Call Accuracy** | 99.9% correct tool selection | Wrong tool, wrong arguments |
| **Response Latency (P95)** | < 5s single-step | Stuck in reasoning loops |
| **Cost Per Task** | < $0.50 mean | Runaway tool loops |
| **Policy Compliance** | 100% adherence | Safety violations |
| **Scope Chain Depth** | â‰¤ 3 hops | Unbounded delegation |
| **Hallucination Rate** | < 1% factual errors | Confident wrong answers |

```python
from agent_sre import SLO, ErrorBudget
from agent_sre.slo.indicators import TaskSuccessRate, CostPerTask, HallucinationRate

slo = SLO(
    name="customer-support-agent",
    indicators=[
        TaskSuccessRate(target=0.995, window="30d"),
        CostPerTask(target_usd=0.50, window="24h"),
        HallucinationRate(target=0.05, window="24h"),
    ],
    error_budget=ErrorBudget(
        total=0.005,
        burn_rate_alert=2.0,      # Alert at 2x normal burn
        burn_rate_critical=10.0,  # Page at 10x burn
    )
)

slo.record_event(good=True)
status = slo.evaluate()  # HEALTHY | WARNING | CRITICAL | EXHAUSTED
```

### 2. Replay Engine â€” Time-Travel Debugging for Agents

Capture every decision point and replay it exactly:

```python
from agent_sre.replay.capture import TraceCapture, SpanKind, TraceStore

# Capture mode: records all decisions, tool calls, costs
with TraceCapture(agent_id="support-bot-v3", task_input="Refund order #12345") as capture:
    span = capture.start_span("tool_call", SpanKind.TOOL_CALL,
                              input_data={"tool": "lookup_order", "order_id": "12345"})
    span.finish(output={"status": "found", "amount": 49.99}, cost_usd=0.02)

    span = capture.start_span("llm_inference", SpanKind.LLM_INFERENCE,
                              input_data={"prompt": "Process refund for $49.99"})
    span.finish(output={"decision": "approve_refund"}, cost_usd=0.15)

# Save trace, replay later, diff with production
store = TraceStore()
store.save(capture.trace)
```

Features: deterministic replay, trace diffing, counterfactual "what-if" analysis, multi-agent distributed traces, automatic PII redaction.

### 3. Progressive Delivery â€” Ship Agent Changes Safely

```yaml
# agent-sre.yaml â€” GitOps deployment spec
apiVersion: agent-sre/v1
kind: AgentRollout
metadata:
  name: support-bot-v4
spec:
  strategy:
    type: canary
    steps:
      - shadow: 100%     # Route all traffic to v4 in shadow mode
        duration: 1h
        analysis:
          - metric: task_success_rate
            threshold: 0.99
      - canary: 5%        # 5% real traffic to v4
        duration: 2h
        analysis:
          - metric: response_quality_score
            threshold: 0.95
          - metric: cost_per_task
            max_increase: 20%
      - canary: 25%
        duration: 4h
      - canary: 100%      # Full rollout
    rollback:
      automatic: true
      on:
        - error_budget_burn_rate > 5.0
        - policy_violations > 0
        - cost_anomaly_detected
```

### 4. Chaos Engineering â€” Break Agents on Purpose

```python
from agent_sre.chaos.engine import ChaosExperiment, Fault, AbortCondition

experiment = ChaosExperiment(
    name="tool-failure-resilience",
    target_agent="research-agent",
    faults=[
        Fault.tool_timeout("web_search", delay_ms=30_000),
        Fault.tool_error("database_query", error="connection_refused", rate=0.5),
        Fault.llm_latency("openai", p99_ms=15_000),
        Fault.delegation_reject("analyzer", rate=0.1),
    ],
    duration_seconds=1800,
    abort_conditions=[
        AbortCondition(metric="task_success_rate", threshold=0.80, comparator="lte"),
        AbortCondition(metric="cost_per_task", threshold=5.00, comparator="gte"),
    ],
)

experiment.start()
for fault in experiment.faults:
    experiment.inject_fault(fault, applied=True)

resilience = experiment.calculate_resilience(
    baseline_success_rate=0.98,
    experiment_success_rate=0.88,
    recovery_time_ms=2500,
)
print(f"Resilience Score: {resilience.overall:.0f}/100")
```

9 pre-built experiment templates: tool timeout, error storms, LLM degradation, cascading failures, cost explosions, and more.

### 5. Cost Guard â€” Prevent $10K Surprises

```python
from agent_sre.cost.guard import CostGuard

guard = CostGuard(
    per_task_limit=2.00,          # Hard cap per task
    per_agent_daily_limit=100.00, # Per agent per day
    org_monthly_budget=5000.00,   # Organization total
    anomaly_detection=True,       # Alert on unusual patterns
    auto_throttle=True,           # Slow down agents approaching limits
    kill_switch_threshold=0.95,   # Kill at 95% budget
)

# Before each task
allowed, reason = guard.check_task("my-agent", estimated_cost=0.50)
if not allowed:
    print(f"Blocked: {reason}")

# After each task
alerts = guard.record_cost("my-agent", "task-42", cost_usd=0.35)
for alert in alerts:
    print(f"âš ï¸ {alert.severity.value}: {alert.message}")
```

Anomaly detection uses Z-score, IQR, and EWMA methods with severity scoring.

### 6. Incident Manager â€” When Agents Fail in Production

```python
from agent_sre.incidents.detector import IncidentDetector, Signal, SignalType

detector = IncidentDetector(correlation_window_seconds=300)

# Register automated responses
detector.register_response("slo_breach", ["auto_rollback", "notify_oncall"])
detector.register_response("cost_anomaly", ["throttle_agent", "generate_postmortem"])

# Ingest signals from your monitoring
signal = Signal(
    signal_type=SignalType.ERROR_BUDGET_EXHAUSTED,
    source="support-agent",
    message="Error budget consumed â€” freeze deployments",
)

incident = detector.ingest_signal(signal)
if incident:
    print(f"ğŸš¨ {incident.severity.value}: {incident.title}")
```

Features: signal correlation, deduplication, circuit breaker per agent, automated postmortem generation with timeline and action items.

---

## Ecosystem Integration

Agent SRE completes the governance-to-reliability stack:

| Layer | Project | What It Does |
|---|---|---|
| **Reliability** | **Agent SRE** (this) | SLOs, chaos testing, canary deploys, cost guard, replay |
| **Runtime** | [Agent Hypervisor](https://github.com/imran-siddique/agent-hypervisor) | Session isolation, execution rings, saga orchestration |
| **Networking** | [AgentMesh](https://github.com/imran-siddique/agent-mesh) | Identity, trust, routing, delegation |
| **Kernel** | [Agent OS](https://github.com/imran-siddique/agent-os) | Policy enforcement, audit, compliance |

### With Agent OS
- Policy violations â†’ SLO breaches (every violation counts against error budget)
- Audit trail â†’ Replay engine (raw data for deterministic replay)
- Shadow mode â†’ Progressive delivery pipeline

### With AgentMesh
- Trust scores â†’ SLI indicators (mesh trust becomes an SLI)
- Scope chains â†’ Distributed traces (every hop is a span)
- Identity rotation â†’ Deployment events (tracked as reliability events)

### With OpenTelemetry
- Native OTLP export for all SLIs and traces
- Custom semantic conventions for agent-specific telemetry
- Compatible with Grafana, Prometheus, Jaeger, and other OTLP-compatible backends

---

## Architecture

```
agent-sre/
â”œâ”€â”€ src/agent_sre/
â”‚   â”œâ”€â”€ slo/               # SLO definitions, SLI collectors, error budgets
â”‚   â”‚   â”œâ”€â”€ indicators.py  # 7 built-in SLIs (task success, cost, hallucination, etc.)
â”‚   â”‚   â”œâ”€â”€ objectives.py  # SLO engine with burn rate alerts
â”‚   â”‚   â””â”€â”€ dashboard.py   # SLO dashboard with compliance history
â”‚   â”œâ”€â”€ replay/            # Deterministic capture and replay engine
â”‚   â”‚   â”œâ”€â”€ capture.py     # Trace capture with PII redaction
â”‚   â”‚   â”œâ”€â”€ engine.py      # Replay, diff, counterfactual analysis
â”‚   â”‚   â”œâ”€â”€ visualization.py  # Execution graphs, critical path
â”‚   â”‚   â””â”€â”€ distributed.py # Multi-agent trace reconstruction
â”‚   â”œâ”€â”€ delivery/          # Progressive delivery (shadow, canary, rollback)
â”‚   â”‚   â”œâ”€â”€ rollout.py     # Shadow mode, canary rollouts, traffic splitting
â”‚   â”‚   â””â”€â”€ gitops.py      # Declarative rollout specs (YAML)
â”‚   â”œâ”€â”€ chaos/             # Chaos engineering and fault injection
â”‚   â”‚   â”œâ”€â”€ engine.py      # Experiment state machine, resilience scoring
â”‚   â”‚   â””â”€â”€ library.py     # 9 pre-built experiment templates
â”‚   â”œâ”€â”€ cost/              # Cost tracking, budgets, anomaly detection
â”‚   â”‚   â”œâ”€â”€ guard.py       # Hierarchical budgets, auto-throttle, kill switch
â”‚   â”‚   â””â”€â”€ anomaly.py     # Z-score, IQR, EWMA anomaly detection
â”‚   â”œâ”€â”€ incidents/         # Detection, response, postmortem generation
â”‚   â”‚   â”œâ”€â”€ detector.py    # Signal correlation, deduplication, routing
â”‚   â”‚   â”œâ”€â”€ circuit_breaker.py  # Per-agent circuit breaker (CLOSED/OPEN/HALF_OPEN)
â”‚   â”‚   â””â”€â”€ postmortem.py  # Automated postmortem with timeline + action items
â”‚   â”œâ”€â”€ integrations/      # Ecosystem bridges
â”‚   â”‚   â”œâ”€â”€ agent_os/      # Agent OS policy + audit â†’ SLI bridge
â”‚   â”‚   â”œâ”€â”€ agent_mesh/    # AgentMesh trust score â†’ SLI bridge
â”‚   â”‚   â”œâ”€â”€ otel/          # OpenTelemetry export
â”‚   â”‚   â”œâ”€â”€ langchain/     # LangChain callback handler
â”‚   â”‚   â”œâ”€â”€ llamaindex/    # LlamaIndex callback handler
â”‚   â”‚   â”œâ”€â”€ langfuse/      # Langfuse SLO scoring + cost export
â”‚   â”‚   â”œâ”€â”€ langsmith/     # LangSmith trace + feedback export
â”‚   â”‚   â”œâ”€â”€ arize/         # Arize/Phoenix span export
â”‚   â”‚   â”œâ”€â”€ braintrust/    # Braintrust eval + experiment export
â”‚   â”‚   â”œâ”€â”€ helicone/      # Helicone header injection + logging
â”‚   â”‚   â”œâ”€â”€ datadog/       # Datadog metrics + events export
â”‚   â”‚   â”œâ”€â”€ agentops/      # AgentOps session + event recording
â”‚   â”‚   â”œâ”€â”€ prometheus/    # Prometheus /metrics text format
â”‚   â”‚   â””â”€â”€ mcp/           # MCP drift detection
â”‚   â”œâ”€â”€ mcp/               # MCP server (agent self-monitoring tools)
â”‚   â”œâ”€â”€ cli/               # CLI tool (agent-sre command)
â”‚   â””â”€â”€ alerts/            # Webhook alerting (Slack, PagerDuty, OpsGenie, Teams)
â”œâ”€â”€ dashboards/            # Pre-built Grafana dashboards
â”œâ”€â”€ operator/              # Kubernetes CRDs (AgentSLO, CostBudget)
â”œâ”€â”€ .github/actions/       # GitHub Actions (canary deployment)
â”œâ”€â”€ examples/              # 4 runnable demos
â”œâ”€â”€ tests/                 # 878 tests
â”œâ”€â”€ docs/                  # Getting started, concepts, integration guide
â””â”€â”€ specs/                 # SLO templates (coming soon)
```

---

## How It Differs

**Observability tools** (LangSmith, Langfuse, Arize) tell you *what happened*.
Agent SRE tells you *if it was within budget* and *what to do about it*.

| | Observability Tools | Agent SRE |
|---|---|---|
| Tracing | âœ… Core strength | âœ… Trace capture + deterministic replay |
| Evaluation | âœ… LLM-as-judge | âœ… SLI recording |
| **SLOs & Error Budgets** | âŒ | âœ… Define reliability targets |
| **Canary Deployments** | âŒ | âœ… Compare agent versions safely |
| **Chaos Testing** | âŒ | âœ… Inject faults, measure resilience |
| **Cost Guardrails** | âŒ (cost tracking only) | âœ… Per-task limits, auto-throttle, kill switch |
| **Incident Detection** | âŒ | âœ… SLO breach â†’ auto-incident â†’ postmortem |
| **Progressive Rollout** | âŒ | âœ… Shadow mode, traffic splitting, rollback |

**Use both together:** observability for deep trace debugging, Agent SRE for production reliability operations.

**AI-powered SRE tools** (Cleric, Resolve, SRE.ai) use AI to help humans debug *infrastructure*. Agent SRE applies SRE principles *to AI agent systems*. Completely different target.

**Traditional APM** (Prometheus, Grafana, Jaeger) monitors infrastructure. Your dashboard says "HTTP 200, latency 150ms, all green" while your agent just approved a fraudulent transaction. Agent SRE catches reasoning failures, not infrastructure failures.

---

## Status & Maturity

### âœ… Fully Implemented (20,000+ lines, 878 tests)

| Component | Status | Description |
|---|---|---|
| **SLO Engine** | âœ… Stable | 7 SLI types, error budgets, burn rate alerts, auto-fire to AlertManager |
| **Replay Engine** | âœ… Stable | Capture, replay, diff, counterfactual, distributed traces |
| **Progressive Delivery** | âœ… Stable | Shadow mode, canary rollouts, analysis gates, auto-rollback |
| **Chaos Engine** | âœ… Stable | 9 fault templates, resilience scoring, abort conditions |
| **Cost Guard** | âœ… Stable | Hierarchical budgets, anomaly detection, auto-throttle |
| **Incident Manager** | âœ… Stable | Signal correlation, circuit breaker, automated postmortem |
| **Agent OS Bridge** | âœ… Stable | Policy violations â†’ SLI, audit entries â†’ signals |
| **AgentMesh Bridge** | âœ… Stable | Trust scores â†’ SLI, mesh events â†’ signals |
| **OpenTelemetry** | âœ… Stable | Full span/metric export with OTEL SDK |
| **LangChain Callbacks** | âœ… Stable | Duck-typed callback handler for SLI collection |
| **LlamaIndex Callbacks** | âœ… Stable | Query/retriever/LLM tracking for RAG pipelines |
| **Langfuse** | âœ… Stable | SLO scoring and cost observation export |
| **LangSmith** | âœ… Stable | Run tracing and evaluation feedback export |
| **Arize/Phoenix** | âœ… Stable | Phoenix span export + evaluation import |
| **Braintrust** | âœ… Stable | Eval-driven monitoring and experiment export |
| **Helicone** | âœ… Stable | Header injection for proxy-based cost/latency tracking |
| **Datadog** | âœ… Stable | Metrics and events export for LLM monitoring |
| **AgentOps** | âœ… Stable | Session recording and event tracking |
| **W&B** | âœ… Stable | Experiment tracking with SRE metrics |
| **MLflow** | âœ… Stable | Experiment logging with SLO data |
| **Prometheus** | âœ… Stable | Native `/metrics` endpoint + Grafana dashboards |
| **MCP Drift Detection** | âœ… Stable | Tool schema fingerprinting, change severity classification |
| **MCP Server** | âœ… Stable | Agent self-monitoring tools (SLO check, cost budget, rollout status) |
| **Webhook Alerting** | âœ… Stable | Slack, PagerDuty, OpsGenie, Microsoft Teams + deduplication |
| **Alert Persistence** | âœ… Stable | SQLite-backed alert history for audit trail |
| **Framework Adapters** | âœ… Stable | LangGraph, CrewAI, AutoGen, OpenAI Agents SDK, Semantic Kernel, Dify |
| **CLI Tool** | âœ… Stable | `agent-sre` CLI for SLO status, cost summary, system info |
| **GitHub Actions** | âœ… Stable | Canary deployment action for CI/CD pipelines |
| **K8s CRDs** | âœ… Stable | AgentSLO and CostBudget custom resource definitions |
| **LLM-as-Judge Evals** | âœ… Stable | RulesJudge + JudgeProtocol, 5 criteria, 3 suite presets |
| **SLO Templates** | âœ… Stable | 4 domain-specific templates (support, coding, research, pipeline) |
| **REST API** | âœ… Stable | Zero-dependency HTTP API for SLO status, incidents, cost, traces |
| **Fleet Management** | âœ… Stable | Multi-agent registry, heartbeats, aggregate health, filtering |
| **Helm Chart** | âœ… Stable | Deployment, Service, CRD templates with configurable values |
| **Benchmark Suite** | âœ… Stable | 10 scenarios across 6 categories with scoring and reporting |
| **Certification** | âœ… Stable | Bronze/Silver/Gold reliability tiers with evidence-based evaluation |
| **A/B Testing** | âœ… Stable | Experiment engine with Welch's t-test and traffic splitting |
| **Protocol Tracing** | âœ… Stable | A2A/MCP-aware distributed tracing with W3C context propagation |

---

## Examples

| Example | Description | Command |
|---|---|---|
| [Quickstart](examples/quickstart.py) | SLO + cost + incident in one script | `python examples/quickstart.py` |
| [LangChain Monitor](examples/langchain_monitor.py) | LangChain RAG agent with SLOs + evals | `python examples/langchain_monitor.py` |
| [Cost Guard](examples/cost_guard.py) | Budget enforcement with throttling | `python examples/cost_guard.py` |
| [Canary Rollout](examples/canary_rollout.py) | Shadow + canary with auto-rollback | `python examples/canary_rollout.py` |
| [Chaos Test](examples/chaos_test.py) | Fault injection and resilience scoring | `python examples/chaos_test.py` |

**Docker:**

```bash
docker compose up quickstart          # Quick demo
docker compose up langchain-monitor   # LangChain + SLOs + LLM-as-Judge
docker compose up api                 # REST API on port 8080
```

**Kubernetes:**

```bash
helm install agent-sre ./deployments/helm/agent-sre
```

### REST API

Full FastAPI REST API with 27 endpoints and interactive Swagger docs:

```bash
pip install agent-sre[api]
uvicorn agent_sre.api.server:app
# Open http://localhost:8000/docs for Swagger UI
```

Endpoints: SLOs, Cost, Chaos, Incidents, Delivery, Health, Metrics.

### Visualization Dashboard

Interactive Streamlit dashboard with 5 tabs:

```bash
cd examples/dashboard
pip install -r requirements.txt
streamlit run app.py
```

Tabs: SLO Health | Cost Management | Chaos Engineering | Incidents | Progressive Delivery

---

## Documentation

- [Getting Started](docs/getting-started.md) â€” Install and define your first SLO in 5 minutes
- [Deployment Guide](docs/deployment.md) â€” Docker, integration patterns, production checklist
- [Security Model](docs/security.md) â€” Threat model, attack vectors, best practices
- [Concepts](docs/concepts.md) â€” Why agent reliability is different from infrastructure reliability
- [Integration Guide](docs/integration-guide.md) â€” Use with Agent OS, AgentMesh, and OpenTelemetry
- [Comparison](docs/comparison.md) â€” Detailed comparison with other tools

---

## Frequently Asked Questions

**Why do AI agents need SRE?**
AI agents in production are services that can fail, degrade, or cost too much -- just like any other service. Agent SRE applies proven Site Reliability Engineering practices (SLOs, error budgets, chaos testing, canary deploys) specifically to AI agent systems, catching reliability issues before they impact users.

**How does chaos engineering work for AI agents?**
Agent SRE injects failures like increased latency, dropped responses, corrupted outputs, and resource exhaustion at specific points in agent workflows. It measures impact on SLOs, triggers automated rollbacks when error budgets are exceeded, and provides replay debugging to analyze failure cascades.

**What SLOs can I define for AI agents?**
Agent SRE supports SLOs for response time, accuracy, cost per inference, safety compliance, and custom metrics. Each SLO has an error budget that burns down when violated. Burn rate alerts notify you before the budget is exhausted, enabling proactive intervention.

**How does Agent SRE integrate with existing monitoring?**
Agent SRE exports metrics via OpenTelemetry and Prometheus. It works alongside your existing Grafana dashboards, PagerDuty alerts, and observability stack. It's part of the [Agent Governance Ecosystem](https://github.com/imran-siddique/agent-os) with 4,310+ tests across 4 repos.

---

## Contributing

```bash
git clone https://github.com/imran-siddique/agent-sre.git
cd agent-sre
pip install -e ".[dev]"
pytest
```

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸ—ºï¸ Roadmap

| Quarter | Milestone |
|---------|-----------|
| **Q1 2026** | âœ… Core 7 engines, OTel integration, Prometheus dashboards |
| **Q2 2026** | Kubernetes operator, PagerDuty/OpsGenie integration |
| **Q3 2026** | ML-powered anomaly detection, auto-remediation |
| **Q4 2026** | Managed cloud service, SOC2 compliance automation |

## License

MIT â€” See [LICENSE](LICENSE) for details.

---

<div align="center">

**Observability tells you what happened. Agent SRE tells you if it was within budget.**

[GitHub](https://github.com/imran-siddique/agent-sre) Â· [Docs](docs/) Â· [Agent OS](https://github.com/imran-siddique/agent-os) Â· [AgentMesh](https://github.com/imran-siddique/agent-mesh) Â· [Agent Hypervisor](https://github.com/imran-siddique/agent-hypervisor)

</div>
